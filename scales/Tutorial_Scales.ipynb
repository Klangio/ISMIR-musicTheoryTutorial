{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ab7b26",
   "metadata": {},
   "source": [
    "# ISMIR 2021 -- Scales, chords, and cadences: Practical music theory for MIR Researchers\n",
    "# Session 1 -- Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728e9c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## 1 Scale Identification (Key Finding) -- Major/Minor\n",
    "### Music21 -- Symbolic Music\n",
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6cf8bb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyACA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qm/x0jnv3rn5nggh1kkgswgjjlc0000gn/T/ipykernel_18660/376771865.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m      \u001b[0;31m# rotate function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mipd\u001b[0m      \u001b[0;31m# audio playback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyACA\u001b[0m                       \u001b[0;31m# audio content analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#from IPython.display import Image  # display png files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpearsonr\u001b[0m   \u001b[0;31m# Pearson correlation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyACA'"
     ]
    }
   ],
   "source": [
    "from music21 import *              # analysis package for symbolic data\n",
    "import numpy as np                 # matrix computing\n",
    "import librosa                     # music/audio package \n",
    "import librosa.display             # for librosa chroma plot\n",
    "import matplotlib.pyplot as plt    # plotting\n",
    "import seaborn as sns              # data visualization based on matplotlib\n",
    "from collections import deque      # rotate function\n",
    "import IPython.display as ipd      # audio playback\n",
    "import pyACA                       # audio content analysis\n",
    "#from IPython.display import Image  # display png files\n",
    "from scipy.stats import pearsonr   # Pearson correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e3d5b",
   "metadata": {},
   "source": [
    "#### 1.1 Example 1 -- TAVERN, K. 573\n",
    "Let's import the opening phrase from the first movement of Mozart's K. 573, encoded in .krn in the TAVERN data set. (You can download TAVERN here: <a href=\"https://github.com/jcdevaney/TAVERN\" target=\"_blank\">https://github.com/jcdevaney/TAVERN</a>.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3784b5e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'converter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-98dbfa4a8842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtheme1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"M573_00_01a_a.krn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'converter' is not defined"
     ]
    }
   ],
   "source": [
    "theme1 = converter.parse(\"M573_00_01a_a.krn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a35b64",
   "metadata": {},
   "source": [
    "Now let's visualize the score using Music21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8543da",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34aeb56",
   "metadata": {},
   "source": [
    "One typical approach to identifying the operative key/scale is to compare a 0th-order distribution of pitch classes to the Krumhansl-Kessler (1982) key profiles. Here's the major-key profile for C major."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca377582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS algorithm.\n",
    "ks = analysis.discrete.KrumhanslSchmuckler()\n",
    "maj = ks.getWeights('major')\n",
    "min1 = ks.getWeights('minor')\n",
    "pcs = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "\n",
    "# Plot KS major-key figure.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.plot(pcs,maj)\n",
    "ax.plot(pcs,min1)\n",
    "ax.set_ylabel('Degree of Fit')\n",
    "ax.set_xlabel('Pitch Classes')\n",
    "plt.ylim([1,7])\n",
    "plt.title('KS Major-Key Profile (1982)')\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a15621",
   "metadata": {},
   "source": [
    "Now let's compute the distribution of pitch classes found in the Mozart theme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb86b0-2ed9-4141-93c3-06b148923d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count PCs.\n",
    "cts = {'C':0, 'C#':0, 'D':0, 'D#':0, 'E':0, 'F':0, 'F#':0, 'G':0, 'G#':0, 'A':0, 'A#/Bb':0, 'B':0}\n",
    "PC = analysis.pitchAnalysis.pitchAttributeCount(theme1,'name')\n",
    "cts.update(PC)\n",
    "\n",
    "# Plot.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(cts.keys(),cts.values())\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('Pitch Classes')\n",
    "plt.title('PC Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e9773",
   "metadata": {},
   "source": [
    "Unsurprisingly, the PC content emphasizes scale-degrees associated with the D major scale. The KS algorithm identifies the key/scale of an excerpt by correlating the major and minor-key profiles for every starting pitch class with the distribution of pitch classes in the excerpt. Let's take this approach for our example and print the correlation coefficient for the most correlated key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1563ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = theme1.analyze('key.krumhanslschmuckler')\n",
    "print(key)\n",
    "key.correlationCoefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22baaf15-8ab6-47be-8178-424716977ad8",
   "metadata": {},
   "source": [
    "Manual calculation of the correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb971407-f061-4e32-a9dd-9beddf28f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = list(cts.values())\n",
    "val = np.roll(tmp,-2) # shift to D major at starting position\n",
    "cor = pearsonr(maj,val)\n",
    "print('D major')\n",
    "print(cor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f74d595",
   "metadata": {},
   "source": [
    "The KS algorithm matches our intuitions! Here's the D major scale for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = stream.Measure()\n",
    "m1.timeSignature = meter.TimeSignature('4/4')\n",
    "m1.keySignature = key.KeySignature(2)\n",
    "m1.append([note.Note('D'), note.Note('E'), note.Note('F#'), note.Note('G')])\n",
    "m2 = stream.Measure()\n",
    "m2.append([note.Note('A'),note.Note('B'),note.Note('C#5'),note.Note('D5')])\n",
    "p = stream.Part()\n",
    "p.append([m1, m2])\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c43036",
   "metadata": {},
   "source": [
    "#### 1.2 Example 2 -- TAVERN, K. 501\n",
    "What about for an excerpt that modulates from one key to another? Let's look at the opening phrase from Mozart, K. 501, which tonicizes (or modulates to) the key of the dominant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c51009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme2 = converter.parse(\"M501_00_01a_a.krn\")\n",
    "theme2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b70cce",
   "metadata": {},
   "source": [
    "What's the key across the entire excerpt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count PCs.\n",
    "cts2 = {'C':0, 'C#':0, 'D':0, 'D#':0, 'E':0, 'F':0, 'F#':0, 'G':0, 'G#':0, 'A':0, 'A#/Bb':0, 'B':0}\n",
    "PC = analysis.pitchAnalysis.pitchAttributeCount(theme2,'name')\n",
    "cts2.update(PC)\n",
    "\n",
    "# KS algorithm\n",
    "key = theme2.analyze('key.krumhanslschmuckler')\n",
    "print(key)\n",
    "key.correlationCoefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcdea77",
   "metadata": {},
   "source": [
    "The excerpt begins in G major, but the KS algorithm predicts that the entire excerpt is in D major instead. We can address this issue by performing a windowed analysis using the KS algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key finding over windows of various sizes (1-12 quarter-note beats).\n",
    "p = graph.plot.WindowedKey(theme2)\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0396f886",
   "metadata": {},
   "source": [
    "***\n",
    "## 2 Scale Identification (Key Finding) -- Major/Minor \n",
    "### librosa -- Audio\n",
    "#### 2.1 Example 1 -- TAVERN, K. 573\n",
    "Sanity check: Let's try all of this again for the audio representation. You can get the audio for the first example here: https://imslp.org/wiki/9_Variations_on_a_Minuet_by_Duport%2C_K.573_(Mozart%2C_Wolfgang_Amadeus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090866c",
   "metadata": {},
   "source": [
    "The windowed analysis is better, but still doesn't really conform to musical intuitions. Perhaps listeners use more than 0th-order statistics to determine the key?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba1f97-5bab-4b23-8e0a-f1e3dd0fdbfc",
   "metadata": {},
   "source": [
    "Play the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738bc384",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'K573_themeA.wav'\n",
    "sig , sr = librosa.load(file,mono=True,sr=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62738b04",
   "metadata": {},
   "source": [
    "Separate the harmonic and percussive components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a43ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428da249",
   "metadata": {},
   "source": [
    "Now let's estimate a chromagram using librosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36670d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_harmonic, sig_percussive = librosa.effects.hpss(sig)\n",
    "chroma = librosa.feature.chroma_cqt(sig_harmonic,sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086706f0",
   "metadata": {},
   "source": [
    "Plot the chroma vector and compare it with the PC distribution we plotted earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "librosa.display.specshow(chroma, y_axis='chroma', x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('Power spectrum chromagram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3315ec8e",
   "metadata": {},
   "source": [
    "Let's compare this chroma vector to the KS major- and minor-key profiles using pyACA. (Note that pyACA uses a distance metric (Manhattan distance) instead of a correlation coefficient, so we'll have to compute the Pearson correlation manually.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average energy in each chroma bin.\n",
    "chroma_mean = np.mean(chroma,axis=1)\n",
    "\n",
    "# Replot PC histogram from symbolic representation.\n",
    "fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "axs[0].bar(cts.keys(),cts.values())\n",
    "axs[0].set_ylabel('Count')\n",
    "axs[0].set_xlabel('Pitch Classes')\n",
    "axs[0].set_title('Symbolic')\n",
    "\n",
    "# Plot Chroma histogram from audio representation.\n",
    "axs[1].bar(pcs,chroma_mean)\n",
    "axs[1].set_ylabel('Proportion')\n",
    "axs[1].set_xlabel('Pitch Classes')\n",
    "axs[1].set_title('Audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28638edf-3c9b-4561-82b0-a6d5b6ee01bc",
   "metadata": {},
   "source": [
    "#### 2.2 Example 1 -- TAVERN, K. 501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate key using pyACA.\n",
    "key = pyACA.computeKey(sig_harmonic,sr)\n",
    "print(key[0])\n",
    "\n",
    "# Correlate manually in order to compare with the symbolic example.\n",
    "chromas = np.roll(chroma_mean,-2) # shift to D major at starting position\n",
    "cor = pearsonr(maj,chromas)\n",
    "print(cor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ffbe4a",
   "metadata": {},
   "source": [
    "How will this method fare for K 501? (There isn't a CC license for K. 501 on imslp.org, so the audio recording is not included in the tutorial, but I'll use my own copy for demonstration purposes.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33334bfa",
   "metadata": {},
   "source": [
    "#### 1.2.2 Example 2 -- TAVERN, K. 501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import file.\n",
    "file = 'K501_themeA.wav'\n",
    "sig, sr = librosa.load(file,mono=True,sr=None)\n",
    "\n",
    "# Separate harmonic and percussive components.\n",
    "sig_harmonic, sig_percussive = librosa.effects.hpss(sig)\n",
    "chroma = librosa.feature.chroma_cqt(sig_harmonic,sr)\n",
    "\n",
    "# Calculate average energy in each chroma bin.\n",
    "chroma_mean = np.mean(chroma,axis=1)\n",
    "\n",
    "# Replot PC histogram from symbolic representation.\n",
    "fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "axs[0].bar(cts2.keys(),cts2.values())\n",
    "axs[0].set_ylabel('Count')\n",
    "axs[0].set_xlabel('Pitch Classes')\n",
    "axs[0].set_title('Symbolic')\n",
    "\n",
    "# Plot Chroma histogram from audio representation.\n",
    "axs[1].bar(pcs,chroma_mean)\n",
    "axs[1].set_ylabel('Proportion')\n",
    "axs[1].set_xlabel('Pitch Classes')\n",
    "axs[1].set_title('Audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c375ded-e2e1-4f5e-b47b-a65f52b48f21",
   "metadata": {},
   "source": [
    "Clearly key-finding for the audio representation is more challenging!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95290ceb",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 3 Scale Identification (Key Finding) -- Beyond Major/Minor\n",
    "### librosa -- Audio\n",
    "#### 3.1 The Who\n",
    "\n",
    "These problems compound when we look beyond common-practice music. Many of the popular music traditions encountered on the radio reflect other scale systems. The Who's \"Won't Get Fooled Again\" is one such example. Let's listen to an excerpt. (There is no CC license for this song, so I haven't included the file in the tutorial, but I'll play it here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate key using pyACA.\n",
    "key = pyACA.computeKey(sig_harmonic,sr)\n",
    "print(key[0])\n",
    "\n",
    "# Correlate manually in order to compare with the symbolic example.\n",
    "chromas = np.roll(chroma_mean,-11) # shift to G major at starting position\n",
    "cor = pearsonr(min1,chromas)\n",
    "print(cor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d68720",
   "metadata": {},
   "outputs": [],
   "source": [
    "who = 'TheWho_WontGetFooledAgain.mp3'\n",
    "sig,sr = librosa.load(file,mono=True,sr=None,duration=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import file.\n",
    "sig, sr = librosa.load('TheWho_WontGetFooledAgain.wav',mono=True,sr=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6efd49",
   "metadata": {},
   "source": [
    "Play the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(sig,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea407b70",
   "metadata": {},
   "source": [
    "Now let's estimate a chromagram using librosa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64cefe2",
   "metadata": {},
   "source": [
    "Visualize the chroma vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdbdd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate harmonic and percussive components.\n",
    "sig_harmonic, sig_percussive = librosa.effects.hpss(sig)\n",
    "chroma = librosa.feature.chroma_cqt(sig_harmonic,sr)\n",
    "\n",
    "# Calculate average energy in each chroma bin.\n",
    "chroma_mean = np.mean(chroma,axis=1)\n",
    "\n",
    "# Replot PC histogram from symbolic representation.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(pcs,chroma_mean)\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.set_xlabel('Chromas')\n",
    "ax.set_title('The Who -- Won''t Get Fooled Again')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc025cf",
   "metadata": {},
   "source": [
    "#### 3.2 Annotations -- RS-200\n",
    "Rather than work from the audio representation, let's look at the chord annotations for this song in the RollingStone-200 data set: http://rockcorpus.midside.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c432d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open('wont_get_fooled_again_dt.har')\n",
    "file_content = text_file.read()\n",
    "print(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c486508",
   "metadata": {},
   "source": [
    "Here's a plot of the scale-degree content extracted from the chord labels in the RollingStone-200 data set for this song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee38b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='TheWho_SDdistribution.png',width=500,height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06aa0fc",
   "metadata": {},
   "source": [
    "This looks like the mixolydian mode! What about if we only count scale degrees if they appear in the lowest (bass) voice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14bf85",
   "metadata": {},
   "source": [
    "We can expand this discussion to look at the scale systems found in the Billboard and RS-200 data sets using a topic model that identifies topics reflected in the chord annotations. When we visualize the scale-degree content found in those topics, distinct scale systems emerge. (Check out my late-breaking demo with Justin Glosson for further details! https://archives.ismir.net/ismir2021/latebreaking/000048.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763dfc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='CPvsPop_Scales.png',width=900,height=500)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "beb8ae63a74e57b2145d688a52e22bfb5105f39ea9e8338900fb11620b79bce8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('3.7.3': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
